{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19434c70",
   "metadata": {},
   "source": [
    "### xpath\n",
    "\n",
    "### Ex 1: \n",
    "\n",
    "Your job will be to create an `XPath` string using `single` forward-slashes and brackets which navigates to the paragraph `p` element which contains the text `\"Where am I?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec211ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Good Luck!</p>\n",
    "      <p>Not here...</p>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Where am I?</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html> '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = \"/html/body/div[2]/p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07d5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1b326",
   "metadata": {},
   "source": [
    "### Ex 2:\n",
    "\n",
    "In this exercise, you will select all paragraph `p` elements within the HTML. Because we want you to navigate to `all paragraph` elements, it is not important that you know what the HTML code is, since the task can be accomplished with a simple XPath string using the double forward-slash notation you have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_all_p = \"//p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d660d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "\n",
    "<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Good Luck!</p>\n",
    "      <p>Not here...</p>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Where am I?</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html> \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a0f2e",
   "metadata": {},
   "source": [
    "### Slashes and Brackets\n",
    "\n",
    "-Single forward slash `/` looks forward `one` generation\n",
    "\n",
    "-Double forward slash `//` looks forward `all` future generations\n",
    "\n",
    "-Square brackets `[]` help narrow in on `specfic` elements\n",
    "\n",
    "\n",
    "`Tips`:\n",
    "\n",
    "1.The number of elements selected with the XPath string `xpath = \"/html/body/*\"` is equal to the `number of children` of the `body element`; whereas the number of elements selected with the XPath string `xpath = \"/html/body//*\"` is equal to the `total number of descendants` of the `body element`.\n",
    "\n",
    "2.The number of elements selected by the XPath string `xpath = \"/*\"` is equal to the number of `root elements` within the `HTML document`, which is typically the `1 html root element`.\n",
    "\n",
    "3.The number of elements selected by the Xpath string `xpath = \"//*\"` is equal to the `total number of elements` in the `entire HTML document`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187435a",
   "metadata": {},
   "source": [
    "### Ex 3:\n",
    "\n",
    "In this exercise, we want to give you the opportunity to create your own `XPath` string to achieve a certain task; the task is to select the paragraph element containing the text `\"Choose DataCamp!\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ed3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Consider the following HTML:\n",
    "\n",
    "<html>\n",
    "  <body>\n",
    "    <div>\n",
    "      <p>Hello World!</p>\n",
    "      <div>\n",
    "        <p>Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div>\n",
    "      <p>Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = \"/html/body/div/div/p\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281a230",
   "metadata": {},
   "source": [
    "### Attribute\n",
    "\n",
    "`@ `represents `\"attribute\"`\n",
    "\n",
    "For example: @class, @id, @href\n",
    "\n",
    "\n",
    "### Ex 4:\n",
    "\n",
    "In this exercise, you'll begin to write an XPath string using attributes to achieve a certain task; that task is to select the paragraph element containing the texts `Hello World!`, `Choose DataCamp!` and `Thanks for Watching!`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Consider the following HTML:\n",
    "\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose DataCamp!</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472009cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hello world \n",
    "\n",
    "xpath1 = '//*[@class=\"class-1\"]/p' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose Datacamp\n",
    "\n",
    "xpath2 = '//*[@id=\"p2\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73151f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thanks for watching\n",
    "\n",
    "xpath3 = '//*[@id=\"div3\"]/p'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4cc5d",
   "metadata": {},
   "source": [
    "### Hyper(link) Active\n",
    "\n",
    "One of the most important attributes to extract for `\"web-crawling\"` is the hyperlink url `(href attribute)` within an a tag. Here, you will extract such a hyperlink!\n",
    "\n",
    "### Ex 5:\n",
    "\n",
    "Complete the variable xpath to select the `href` attribute value from the `DataCamp` hyperlink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The exercise refers to the following HTML source code:\n",
    "\n",
    "<html>\n",
    "  <body>\n",
    "    <div id=\"div1\" class=\"class-1\">\n",
    "      <p class=\"class-1 class-2\">Hello World!</p>\n",
    "      <div id=\"div2\">\n",
    "        <p id=\"p2\" class=\"class-2\">Choose \n",
    "            <a href=\"http://datacamp.com\">DataCamp!</a>!\n",
    "        </p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div id=\"div3\" class=\"class-2\">\n",
    "      <p class=\"class-2\">Thanks for Watching!</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//*[@id=\"p2\"]/a/@href'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ac2c3",
   "metadata": {},
   "source": [
    "### Contains\n",
    "\n",
    "Xpath `Contains` Notation:\n",
    "\n",
    "`contains(@attribute-name, \"string-expression\")`\n",
    "\n",
    "### Ex 6:\n",
    "\n",
    "Assign an XPath string to the variable `xpath` which directs to all `href` attribute values of the hyperlink a elements whose `class` attributes `contain` the string `\"package-snippet\"`. Remember that we use the `contains` call within the XPath string to check if an attribute value contains a particular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = '//a[contains(@class, \"package-snippet\")]/@href'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1a48a",
   "metadata": {},
   "source": [
    "### CSS Locators\n",
    "\n",
    "1.`/` replace by `>` (`except` the `first` character)\n",
    "\n",
    "XPath: `/html/body/div`\n",
    "\n",
    "CSS Locator:`html > body > div`\n",
    "\n",
    "2.`//` replaced by a `blank` space(`except` the `first` character) \n",
    "\n",
    "XPath: `//div/span//p`\n",
    "\n",
    "CSS Locator: `div > span p`\n",
    "\n",
    "3.`[N]` replaced by `:nth-of-type(N)`\n",
    "\n",
    "XPath: `//div/p[2]`\n",
    "\n",
    "CSS Locator: `div > p:nth-of-type(2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4f7ed",
   "metadata": {},
   "source": [
    "### Ex 7: The (X)Path to CSS Locators\n",
    "\n",
    "Assign to the variable `css_locator` a CSS Locator string which is equivalent to the `XPath` string given.\n",
    "\n",
    "`xpath = '/html/body/span[1]//a'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the CSS Locator string equivalent to the XPath\n",
    "\n",
    "css_locator = \"html > body > span:nth-of-type(1) a\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542097d",
   "metadata": {},
   "source": [
    "### Ex 8: \n",
    "\n",
    "Assign to the variable xpath a `XPath` string which is equivalent to the CSS Locator string given.\n",
    "\n",
    "`css_locator = 'div#uid > span h4'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the XPath string equivalent to the CSS Locator \n",
    "\n",
    "Xpath = '//div[@id=\"uid\"]/span//h4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962366f",
   "metadata": {},
   "source": [
    "### The CSS Wildcard\n",
    "\n",
    "You can use the wildcard `*` in CSS Locators too! In fact, we can use it in a similar way, when we want to ignore the tag type. For example:\n",
    "\n",
    "1.The CSS Locator string `'*'` selects `all elements` in the HTML document.\n",
    "\n",
    "2.The CSS Locator string `'*.class-1'` selects `all elements which belong to class-1`, but this is unnecessary since the string `'.class-1'` will also do the `same` job.\n",
    "\n",
    "3.The CSS Locator string `'*#uid'` selects the `element with id attribute equal to uid`, but this is unnecessary since the string `'#uid'` will also do the same job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2eceaa",
   "metadata": {},
   "source": [
    "### Ex 9:\n",
    "\n",
    "Assign to the variable `css_locator` a CSS Locator string which will select `all children` (regardless of tag-type) of the unique element in the HTML document that has its `id attribute` equal to `uid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c68f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the CSS Locator to all children of the element whose id is uid\n",
    "\n",
    "css_locator = '#uid > *'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d70b98d",
   "metadata": {},
   "source": [
    "### XPath and CSS locator Chaining\n",
    "\n",
    "Selector and SelectorList objects allow for chaining when using the xpath method. What this means is that you can apply the xpath method over once you've already applied it. For example, if sel is the name of our Selector, then-\n",
    "\n",
    "`sel.xpath('/html/body/div[2]')`\n",
    "\n",
    "is the same as--\n",
    "\n",
    "`sel.xpath('/html').xpath('./body/div[2]')`\n",
    "\n",
    "or is the same as--\n",
    "\n",
    "`sel.xpath('/html').xpath('./body').xpath('./div[2]')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f0978",
   "metadata": {},
   "source": [
    "### Ex 10:\n",
    "\n",
    "1.Assign the variable `css_locator` a CSS Locator string which directs to the `hyperlink (a element)` children of `all div element`s belonging to the class `\"course-block\"`.\n",
    "\n",
    "2.Assign to the variable `hrefs_from_xpath` the `href` attribute values from the elements in `course_as`. \n",
    "\n",
    "3.Do the same for CSS locator `hrefs_from_css`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf8aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_as = 'div.course-block > a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a570ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting all href attributes chaining with xpath\n",
    "\n",
    "hrefs_from_xpath = course_as.xpath('./@href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327eca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Selecting all href attributes chaining with css\n",
    "\n",
    "hrefs_from_css = course_as.css('::attr(href)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011f269",
   "metadata": {},
   "source": [
    "### HTML text to Selector\n",
    "\n",
    "We can create a scrapy `Selector` object using a string with the html code.\n",
    "\n",
    "1.The `selector` selects the `entire` html document when it is passed to the `text` argument\n",
    "\n",
    "2.`Selector` and `SelectorList` objects allow for `chaining` when using the `xpath` method. What this means is that you can apply the `xpath` method over once you've already applied it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example\n",
    "\n",
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"hello datacamp\">\n",
    "            <p>Hello World!</p>\n",
    "        </div>\n",
    "        <p>Enjoy DataCamp!</p>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab172a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Selector\n",
    "# The selector sel has selected the entire html document\n",
    "\n",
    "sel = Selector( text = html )\n",
    "\n",
    "# Selecting Selectors\n",
    "# We can use the xpath call within a Selector to create new Selectors of specific pieces of the html code\n",
    "# The return is a SelectorList of Selector objects\n",
    "\n",
    "print(\"Selector List: \\n\",sel.xpath(\"//p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Extracting Data from a SelectorList, Use the .extract() method\n",
    "\n",
    "sel.xpath(\"//p\").extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ada7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use extract_first() to get the first element of the list\n",
    "\n",
    "first_p = sel.xpath(\"//p\").extract_first()\n",
    "first_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ba2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way \n",
    "\n",
    "ps = sel.xpath('//p')\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ee020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract by indexing\n",
    "\n",
    "second_p = ps[1]\n",
    "second_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_p.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498b15c",
   "metadata": {},
   "source": [
    "### Ex 11:\n",
    "\n",
    "the URL of Datacamp website in the string variable url and use the requests library to put the content from the website into the string variable html. Your task is to--- \n",
    "\n",
    "1.Create the string `html` containing the HTML source\n",
    "\n",
    "2.Set up the Selector object sel with the `html` variable passed as the `text` argument..\n",
    "\n",
    "3.Print out the number of `elements` in the HTML document\n",
    "\n",
    "4.Create a `SelectorList` of all `div` elements in the HTML document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e657b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://app.datacamp.com/learn/courses\"\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "\n",
    "html = requests.get(url).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "\n",
    "sel = Selector(text=html)\n",
    "\n",
    "print(f\"You have found total {len(sel.xpath('//*'))} elements\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb99835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SelectorList of all div elements in the HTML document\n",
    "\n",
    "divs = sel.xpath(\"//div\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5173c",
   "metadata": {},
   "source": [
    "### Attribute and Text Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a4404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example \n",
    "html = '''\n",
    "<p id=\"p-example\">\n",
    "    Hello world!\n",
    "    Try <a href=\"http://www.datacamp.com\">DataCamp</a> today!\n",
    "</p>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e1bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In XPath use text(), do not include the text of future generations\n",
    "\n",
    "sel.xpath('//p[@id=\"p-example\"]/text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CSS Locator, use ::text, do not include the text of future generations\n",
    "\n",
    "sel.css('p#p-example::text').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33388203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In XPath use text(), include the text of future generations\n",
    "\n",
    "sel.xpath('//p[@id=\"p-example\"]//text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For CSS Locator, use ::text, include the text of future generations\n",
    "\n",
    "sel.css('p#p-example ::text').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0693496",
   "metadata": {},
   "source": [
    "### Ex 12:\n",
    "\n",
    "1.Assign to the variable xpath an `XPath` string directing to the `text` within the paragraph `p` element with `id` equal to `p3`, which does `not` include the text of `future generations` of this `p` element.\n",
    "\n",
    "2.Assign to the variable `css_locator` a CSS Locator string directing to this same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fc6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an XPath string to the desired text.\n",
    "Xpath = '//p[@id=\"p3\"]/text()'\n",
    "\n",
    "## Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3::text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b951a8",
   "metadata": {},
   "source": [
    "### Ex 13:\n",
    "\n",
    "1.Assign to the variable xpath an `XPath` string directing to the text within the paragraph p element with id equal to p3, which `includes` the `text` of `future generations` of this `p` element.\n",
    "\n",
    "2.Assign to the variable `css_locator` a CSS Locator string directing to this same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an XPath string to the desired text.\n",
    "Xpath = '//p[@id=\"p3\"]//text()'\n",
    "\n",
    "## Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3 ::text'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10502b",
   "metadata": {},
   "source": [
    "### Response:\n",
    "\n",
    "The Response has `all` the tools we learned with Selectors:\n",
    "\n",
    "1.`xpath` and `css` methods and also `extract` and `extract_first` methods. `Chaining` process also works like a `Selector`.\n",
    "\n",
    "2.The response `keeps track` of the `URL` within the `response.url()` method\n",
    "\n",
    "3.The Response helps us move from one site to another, so that we can `\"crawl\"` the web while scraping.\n",
    "\n",
    "4.The response lets us \"follow\" a new link with the `follow()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a8138",
   "metadata": {},
   "source": [
    "### Ex 14:\n",
    "\n",
    "The Response object, named response, is from a `secret` website. Your job is to figure out the `URL` and the `title` of the website using the response variable. To find the website title, what you need to know is:\n",
    "\n",
    "1.The title is the text from the `title` element\n",
    "\n",
    "2.The `title` element is a `child` of the `head` element, which is a `child` of the `html` root element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a26c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.datacamp.com/courses/all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb3f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = Selector(text=html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11825ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  Get the title of the website loaded in response\n",
    "\n",
    "the_title = sel.xpath('/html/head/title/text()').extract_first()\n",
    "the_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f120e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 23:05:31 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
      "2021-11-02 23:05:31 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 20.0.1 (OpenSSL 1.1.1k  25 Mar 2021), cryptography 3.4.7, Platform Windows-10-10.0.18362-SP0\n",
      "2021-11-02 23:05:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2021-11-02 23:05:31 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2021-11-02 23:05:31 [scrapy.extensions.telnet] INFO: Telnet Password: 14af77892de7746a\n",
      "2021-11-02 23:05:31 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-11-02 23:05:32 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-11-02 23:05:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-11-02 23:05:32 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-11-02 23:05:32 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-11-02 23:05:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-11-02 23:05:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-11-02 23:05:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2021-11-02 23:05:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:33 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-11-02 23:05:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 7035,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2464797,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 2.744286,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 11, 2, 17, 5, 34, 870968),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/404': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2021, 11, 2, 17, 5, 32, 126682)}\n",
      "2021-11-02 23:05:34 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.css( \"p.course__description ::text\" )\n",
    "    # Extract the text and strip it clean\n",
    "        crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "        dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "\n",
    "print(url_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f2e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_short = \"https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63936c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaea2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
