{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deee30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6ddb4",
   "metadata": {},
   "source": [
    "### Selecting features for model performance\n",
    "\n",
    "Another, more pragmatic, approach to decide on keeping individual or pairwise properties of features in the dataset or not is to select features based on how they affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7d297d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>family</th>\n",
       "      <th>age</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  diastolic  triceps  insulin   bmi  family  age      test\n",
       "0         1       89         66       23       94  28.1   0.167   21  negative\n",
       "1         0      137         40       35      168  43.1   2.288   33  positive\n",
       "2         3       78         50       32       88  31.0   0.248   26  positive\n",
       "3         2      197         70       45      543  30.5   0.158   53  positive\n",
       "4         1      189         60       23      846  30.1   0.398   59  positive"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"PimaIndians.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c52643d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"test\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c96c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3fbbce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e07bc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b797d",
   "metadata": {},
   "source": [
    "#### Pre-processing the data\n",
    "\n",
    "To train a model on this data we should first perform a train - test split, and in this case also standardize the training feature dataset X_train to have a mean of zero and a variance of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40285666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b577b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5dd76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65b3ce",
   "metadata": {},
   "source": [
    "#### Creating a logistic regression model\n",
    "\n",
    "We can then create and fit a logistic regression model on this standardized training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a828ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8caad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a79dbab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913ce67",
   "metadata": {},
   "source": [
    "To see how the model performs on the test set we first scale these features with the .transform() method of the scaler that we just fit on the training set and then make our prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d5daaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e5b2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "123ce729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80860598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2400702 ,  1.28616614, -0.03981669,  0.17499413, -0.14430102,\n",
       "        0.43329563,  0.31921669,  0.35260952])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a293a",
   "metadata": {},
   "source": [
    "#### Inspecting the feature coefficients\n",
    "\n",
    "However, when we look at the feature coefficients that the logistic regression model uses in its decision function, we'll see that some values are pretty close to zero. \n",
    "\n",
    "Since these coefficients will be multiplied with the feature values when the model makes a prediction, features with coefficients close to zero will contribute little to the end result. \n",
    "\n",
    "We can use the zip function to transform the output into a dictionary that shows which feature has which coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83fea16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.9% accuracy on test set.\n",
      "{'pregnant': 0.24, 'glucose': 1.29, 'diastolic': 0.04, 'triceps': 0.17, 'insulin': 0.14, 'bmi': 0.43, 'family': 0.32, 'age': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# Prints accuracy metrics and feature coefficients\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326dffd",
   "metadata": {},
   "source": [
    "We get almost 80% accuracy on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071bab7",
   "metadata": {},
   "source": [
    "#### Manual Recursive Feature Elimination\n",
    "\n",
    "If we want to remove a feature from the initial dataset with as little effect on the predictions as possible, we should pick the one with the lowest coefficient, 'diastolic' in this case. The fact that we standardized the data first makes sure that we can compare the coefficients to one another.\n",
    "\n",
    "\n",
    "Now we'll remove the feature with the lowest model coefficient from X, in this case we'll remove feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc23a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the feature with the lowest model coefficient--->>> 'diastolic'\n",
    "X = df[['pregnant', 'glucose','triceps', 'insulin', 'bmi', 'family', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2259f288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04cf5c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7add4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.6% accuracy on test set.\n",
      "{'pregnant': 0.05, 'glucose': 1.24, 'triceps': 0.24, 'insulin': 0.2, 'bmi': 0.39, 'family': 0.34, 'age': 0.35}\n"
     ]
    }
   ],
   "source": [
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d4194",
   "metadata": {},
   "source": [
    "We get almost 81% accuracy on the test set.\n",
    "\n",
    "Now we'll remove 2 more features with the lowest model coefficients, which are features - 'pregnant' and 'insulin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0480271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 2 features with the lowest model coefficients---->>> 'pregnant', 'insulin'\n",
    "X = df[['glucose', 'triceps', 'bmi', 'family', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eebf99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d564b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1233e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6% accuracy on test set.\n",
      "{'glucose': 1.13, 'triceps': 0.25, 'bmi': 0.34, 'family': 0.34, 'age': 0.37}\n"
     ]
    }
   ],
   "source": [
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45af26",
   "metadata": {},
   "source": [
    "We get almost 80% accuracy on the test set.\n",
    "\n",
    "Now we'll only keep the feature with the highest coefficient which is 'glucose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9015c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the feature with the highest coefficient\n",
    "X = df[['glucose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c8c289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14ff0863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scales features and fits the logistic regression model to the data\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdab7420",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.5% accuracy on test set.\n",
      "{'glucose': 1.28}\n"
     ]
    }
   ],
   "source": [
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d530de5",
   "metadata": {},
   "source": [
    "We get almost 76% accuracy on the test set with only one feature, that's not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e66dad",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination\n",
    "\n",
    "We could repeat this process until we have the desired number of features remaining, but it turns out, there's a Scikit-learn function that does just that.\n",
    "\n",
    "RFE for \"Recursive Feature Elimination\" is a feature selection algorithm that can be wrapped around any model that produces feature coefficients or feature importance values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c10083d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b2cb6",
   "metadata": {},
   "source": [
    "Here, We can pass it the model we want to use and the number of features we want to select. While fitting to our data it will repeat a process where it first fits the internal model and then drops the feature with the weakest coefficient. It will keep doing this until the desired number of features is reached. \n",
    "\n",
    "If we set RFE's verbose parameter higher than zero we'll be able to see that features are dropped one by one while fitting. \n",
    "\n",
    "We could also decide to just keep the 2 features with the highest coefficients after fitting the model once, but this recursive method is safer, since dropping one feature will cause the other coefficients to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1ca197b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3db2b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"test\", axis = 1)\n",
    "y = df[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a8e26032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "84fafa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0f7e062d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train_std, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e269c9",
   "metadata": {},
   "source": [
    "#### Inspecting the RFE results\n",
    "\n",
    "Once RFE is done, by Using the zip function once more, we can also check out rfe's ranking_ attribute to see in which iteration a feature was dropped. Values of 1 mean that the feature was kept in the dataset until the end while high values mean the feature was dropped early on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e995ecc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregnant': 5, 'glucose': 1, 'diastolic': 6, 'triceps': 3, 'insulin': 4, 'bmi': 1, 'family': 2, 'age': 1}\n"
     ]
    }
   ],
   "source": [
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df2eab0",
   "metadata": {},
   "source": [
    "we can check the support_ attribute that contains True/False values to see which features were kept in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "aa6cb1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5f4f6",
   "metadata": {},
   "source": [
    "Finally, we can test the accuracy of the model with just three remaining features, 'glucose', 'bmi' and 'age', turns out the accuracy is still untouched at 81%. This means the other 5 features had little to no impact on the model an its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5ee5d7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test_std))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91073d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd44bfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddcf54eb",
   "metadata": {},
   "source": [
    "#### Tree-based feature selection\n",
    "\n",
    "Some models will perform feature selection by design to avoid overfitting. One of those, is the random forest classifier. It's an ensemble model that will pass different, random, subsets of features to a number of decision trees. To make a prediction it will aggregate over the predictions of the individual trees.\n",
    "\n",
    "While simple in design, random forests often manage to be highly accurate and avoid overfitting even with the default Scikit-learn settings.\n",
    "\n",
    "\n",
    "By averaging how often features are used to make decisions inside the different decision trees, and taking into account whether these are important decisions near the root of the tree or less important decisions in the smaller branches of the tree, the random forest algorithm manages to calculate feature importance values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c94a8",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "516f769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"test\", axis = 1)\n",
    "y = df[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b7080f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "218a69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e79ff8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "772859bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b4b3a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "6a5c1d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregnant': 0.07, 'glucose': 0.25, 'diastolic': 0.09, 'triceps': 0.09, 'insulin': 0.14, 'bmi': 0.12, 'family': 0.12, 'age': 0.13}\n"
     ]
    }
   ],
   "source": [
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e7fac7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.6% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93575391",
   "metadata": {},
   "source": [
    "#### Feature importance values\n",
    "\n",
    "Feature importance values can be extracted from a trained model using the feature_importances_ attribute. \n",
    "\n",
    "Just like the coefficients produced by the logistic regressor, these feature importance values can be used to perform feature selection, since for unimportant features they will be close to zero. \n",
    "\n",
    "An advantage of these feature importance values over coefficients is that they are comparable between features by default, since they always sum up to one. Which means we don't have to scale our input data first.\n",
    "\n",
    "\n",
    "#### Feature importance as a feature selector\n",
    "\n",
    "We can use the feature importance values to create a True/False mask for features that meet a certain importance threshold. Then, we can apply that mask to our feature dataframe to implement the actual feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "fd7a1235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False,  True, False, False,  True])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.12\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "85ea4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:,mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1ccc1cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'insulin', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# prints out the selected column names\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf16f20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "030571d2",
   "metadata": {},
   "source": [
    "#### RFE with random forests\n",
    "\n",
    "Remember dropping one weak feature can make other features relatively more or less important. If you want to play safe and minimize the risk of dropping the wrong features, you should not drop all least important features at once but rather one by one. To do so we can once again wrap a Recursive Feature Eliminator, or RFE(), around our model. \n",
    "\n",
    "Here, we've reduced the number of features to 3 with no reduction in test set accuracy. However, training the model once for each feature we want to drop can result in too much computational overhead.\n",
    "\n",
    "To speed up the process we can pass the \"step\" parameter to RFE(). Here, we've set it to 2 so that on each iteration the 2 least important features are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e95cc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=3, step = 2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "730fc99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestClassifier(), n_features_to_select=3, step=2,\n",
       "    verbose=1)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de8e2a",
   "metadata": {},
   "source": [
    "Once the final model is trained, we can use the feature eliminator's .support_ attribute as a mask to print the remaining column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e35d5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask\n",
    "mask = rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a5cd7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "66651045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'insulin', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "739b1640",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['glucose', 'insulin', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "276f9e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.21, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a902b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "55945cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a3236630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "73125aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.7% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f9adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ac467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ad279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8dbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972b05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
