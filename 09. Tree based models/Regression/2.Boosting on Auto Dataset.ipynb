{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc4ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c4099e",
   "metadata": {},
   "source": [
    "### The effectiveness of gradual learning\n",
    "\n",
    "Boosting, is based on a technique known as gradual learning.\n",
    "\n",
    "#### Collective learning\n",
    "\n",
    "The ensemble methods are based on an idea known as collective learning - that is, the wisdom of the crowd. \n",
    "\n",
    "- The idea is that, The combined prediction of individual models is superior to any of the individual predictions on their own. \n",
    "\n",
    "- For collective learning to be efficient, the estimators need to be independent and uncorrelated. \n",
    "\n",
    "- In addition, all the estimators are learning the same task, for the same goal: to predict the target variable given the features. \n",
    "\n",
    "- Because the estimators are independent, these can be trained in parallel to speed up the model building. \n",
    "\n",
    "#### Gradual learning\n",
    "\n",
    "Gradual learning methods, on the other hand, are based on the principle of iterative learning. \n",
    "\n",
    "- In this approach, each subsequent model tries to fix the errors of the previous model. \n",
    "\n",
    "- Gradual learning creates dependent estimators, as each model takes advantage of the knowledge from the previous estimator. \n",
    "\n",
    "- In iterative learning, each model is learning a different task, but each one contributes to the same goal of accurately predicting the target variable. \n",
    "\n",
    "- As gradual learning follows a sequential model building process, models cannot be trained in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d0aaa",
   "metadata": {},
   "source": [
    "### AdaBoost properties\n",
    "\n",
    "There are two distinctive properties of Adaptive Boosting compared to other Boosting algorithms. \n",
    "\n",
    "- First, the instances are drawn using a sample distribution of the training data into each subsequent dataset. This sample distribution makes sure that instances which were harder to predict for the previous estimator have a higher chance to be included in the training set for the next estimator by giving them higher weights. The distribution is initialized to be uniform. \n",
    "\n",
    "- Secondly, the estimators are combined through weighted majority voting. The voting weights are based on the estimators training error. Estimators which have shown good performance are rewarded with higher weights for voting. \n",
    "\n",
    "In addition, AdaBoost is guaranteed to improve as the ensemble grows if each estimator has an error rate less than 0.5. In other words, each estimator needs to be a \"weak\" model. And similar to Bagging, AdaBoost can be used for both Classification and Regression with its two variations.\n",
    "\n",
    "#### AdaBoost regressor with scikit-learn\n",
    "\n",
    "We can also find the AdaBoostRegressor class in the scikit-learn ensemble module. To instantiate an AdaBoost regression model, we need to call it with the some parameters.\n",
    "\n",
    "- The parameter base_estimator works as usual, it's the weak model template for all the estimators. If not specified, the default is a Decision Tree regressor with a max depth of 3(1 for classifier), also known as a decision stump. \n",
    "\n",
    "- The second parameter is the number of estimators we want to use. By default is 50. If there's a perfect fit, or an estimator with error higher than 50%, no more estimators are built. \n",
    "\n",
    "- Other important parameter is learning rate, which represents how much each estimator contributes to the ensemble. This is 1.0 by default. In addition, there is a trade-off between the number of estimators and the learning rate.\n",
    "\n",
    "- In addition, we have the loss parameter, which is the function used to update weights. By default, it is linear, but you can also use the square or exponential loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df61dda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>origin</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>US</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>Asia</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>Europe</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  origin  size\n",
       "0  18.0  250.0   88    3139   14.5      US  15.0\n",
       "1   9.0  304.0  193    4732   18.5      US  20.0\n",
       "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
       "3  18.5  250.0   98    3525   19.0      US  15.0\n",
       "4  34.3   97.0   78    2188   15.8  Europe  10.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"auto.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d74f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"mpg\", \"origin\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f290af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4e2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129dfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de9bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89fe9673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Logistic Regression: 5.007\n"
     ]
    }
   ],
   "source": [
    "# Build and fit linear regression model\n",
    "reg_lm = LinearRegression(normalize = True)\n",
    "reg_lm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred_lr = reg_lm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test,pred_lr))\n",
    "print('RMSE Logistic Regression: {:.3f}'.format(rmse_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "600d2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1218a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Decision Tree: 4.365\n"
     ]
    }
   ],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=3,random_state=500)\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Compute y_pred\n",
    "pred_dt = dt.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, pred_dt))\n",
    "print('RMSE Decision Tree: {:.3f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67a4c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "446ff3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE AdaBoost with LR: 4.855\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a normalized linear regression model\n",
    "reg_lm = LinearRegression(normalize = True)\n",
    "\n",
    "# Build and fit an AdaBoost regressor\n",
    "reg_ada = AdaBoostRegressor(base_estimator=reg_lm, n_estimators = 100, random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred_lr_ada = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_lr_ada = np.sqrt(mean_squared_error(y_test, pred_lr_ada))\n",
    "print('RMSE AdaBoost with LR: {:.3f}'.format(rmse_lr_ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cd91e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE AdaBoost with DT : 4.265\n"
     ]
    }
   ],
   "source": [
    "# Build and fit an AdaBoost regressor\n",
    "reg_ada = AdaBoostRegressor(base_estimator=dt, n_estimators=100,  learning_rate=0.1,random_state=500)\n",
    "reg_ada.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred_dt_ada = reg_ada.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_dt_ada = np.sqrt(mean_squared_error(y_test, pred_dt_ada))\n",
    "print('RMSE AdaBoost with DT : {:.3f}'.format(rmse_dt_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f5f24",
   "metadata": {},
   "source": [
    "### Intro to gradient boosting machine\n",
    "\n",
    "To understand the intuition behind Gradient Boosting Machine consider the following- \n",
    "\n",
    "- Suppose that you want to estimate an objective function, let's say y as a function of X. That means, \n",
    "\n",
    "                                        Objective: y = f(X)\n",
    "\n",
    "- On the first iteration, our initial model is a weak estimator that is fit to the dataset. Let's call it f1(x). \n",
    "\n",
    "                          Initial model (weak estimator): y ∼ f1(X)\n",
    "\n",
    "- Then, on each subsequent iteration, a new model is built and fitted to the residual error from the previous iteration. The error is calculated as y minus f1(x). That means, \n",
    "\n",
    "                        New model ,fits to residuals: y − f1(X) ∼ f2(X)\n",
    "\n",
    "- After each individual estimator is built, the result is a new additive model, which is an improvement on the previous estimate. We repeat this process n times or until the error is small enough such that the difference in performance is negligible. \n",
    "\n",
    "- After the algorithm is finished, the result is a final improved additive model. \n",
    "\n",
    "                        Final additive model: y ∼ f1(X) + f2(X) + ... + fn(x)\n",
    "\n",
    "This is a peculiarity of Gradient Boosting, as the individual estimators are not combined through voting or averaging, but by addition. This is because only the first model is fitted to the target variable, and the rest are estimates of the residual errors.\n",
    "\n",
    "\n",
    "### Gradient boosting Regressor\n",
    "\n",
    "To build a Gradient Boosting Regressor, we first import the class from the sklearn ensemble module. This will allow you to instantiate the Gradient Boosting Regressor. Unlike with other ensemble methods, here we don't specify the base_estimator, as Gradient Boosting is implemented and optimized with regression trees as the individual estimators.  \n",
    "\n",
    "- The first parameter is n_estimators, it is 100 by default. \n",
    "\n",
    "- Then, we also specify the learning rate, It is 0.1 by default. \n",
    "\n",
    "- In addition, we have the tree-specific parameters: the maximum depth, which is 3 by default, the minimum number of samples required to split a node, the minimum number of samples required in a leaf node, and the maximum number of features. \n",
    "\n",
    "In Gradient Boosting, it is recommended to use all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a162db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e5e07d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Gradient Boosting: 4.228\n"
     ]
    }
   ],
   "source": [
    "# Build and fit a Gradient Boosting classifier\n",
    "Reg_gbm = GradientBoostingRegressor(n_estimators = 100, learning_rate=0.1, random_state=500)\n",
    "Reg_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions on the test set\n",
    "pred_gbm = Reg_gbm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_gbm = np.sqrt(mean_squared_error(y_test, pred_gbm))\n",
    "print('RMSE Gradient Boosting: {:.3f}'.format(rmse_gbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74eae6",
   "metadata": {},
   "source": [
    "### Variations of gradient boosting\n",
    "\n",
    "In this lesson, you'll learn about some variations, or flavors, of the gradient boosting family of algorithms, along with their implementations in Python.\n",
    "\n",
    "#### Categorical boosting(CatBoost)\n",
    "\n",
    "Categorical Boosting (or CatBoost) is the most recent Gradient Boosting flavor. It was open sourced by Yandex, a Russian tech company, in April 2017. \n",
    "\n",
    "CatBoost has built-in capacity to handle categorical features, so you don't need to do the preprocessing yourself. It is a fast implementation which can scale to large datasets and run on a GPU if required. CatBoost also provides a user friendly interface that integrates well with scikit-learn. \n",
    "\n",
    "To build a CatBoost estimator, we import catboost and give it the alias cb. This gives us access to CatBoostClassifier and CatBoostRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8c21e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d1cec11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 7.1822880\ttotal: 467us\tremaining: 46.3ms\n",
      "1:\tlearn: 6.7767775\ttotal: 896us\tremaining: 43.9ms\n",
      "2:\tlearn: 6.4103424\ttotal: 1.42ms\tremaining: 46.1ms\n",
      "3:\tlearn: 6.0857905\ttotal: 1.87ms\tremaining: 44.9ms\n",
      "4:\tlearn: 5.7591769\ttotal: 2.37ms\tremaining: 45.1ms\n",
      "5:\tlearn: 5.4745471\ttotal: 2.82ms\tremaining: 44.1ms\n",
      "6:\tlearn: 5.2242875\ttotal: 3.19ms\tremaining: 42.3ms\n",
      "7:\tlearn: 4.9832336\ttotal: 3.56ms\tremaining: 40.9ms\n",
      "8:\tlearn: 4.7795984\ttotal: 3.96ms\tremaining: 40.1ms\n",
      "9:\tlearn: 4.6128593\ttotal: 4.32ms\tremaining: 38.9ms\n",
      "10:\tlearn: 4.4647771\ttotal: 4.71ms\tremaining: 38.1ms\n",
      "11:\tlearn: 4.3544540\ttotal: 5.09ms\tremaining: 37.3ms\n",
      "12:\tlearn: 4.2356742\ttotal: 5.45ms\tremaining: 36.5ms\n",
      "13:\tlearn: 4.1338963\ttotal: 5.82ms\tremaining: 35.7ms\n",
      "14:\tlearn: 4.0415002\ttotal: 6.18ms\tremaining: 35ms\n",
      "15:\tlearn: 3.9727747\ttotal: 6.53ms\tremaining: 34.3ms\n",
      "16:\tlearn: 3.9199435\ttotal: 6.89ms\tremaining: 33.6ms\n",
      "17:\tlearn: 3.8522944\ttotal: 7.25ms\tremaining: 33ms\n",
      "18:\tlearn: 3.7997483\ttotal: 7.6ms\tremaining: 32.4ms\n",
      "19:\tlearn: 3.7706091\ttotal: 7.96ms\tremaining: 31.8ms\n",
      "20:\tlearn: 3.7261287\ttotal: 8.32ms\tremaining: 31.3ms\n",
      "21:\tlearn: 3.6955234\ttotal: 8.67ms\tremaining: 30.7ms\n",
      "22:\tlearn: 3.6608301\ttotal: 9.03ms\tremaining: 30.2ms\n",
      "23:\tlearn: 3.6403739\ttotal: 9.38ms\tremaining: 29.7ms\n",
      "24:\tlearn: 3.6137123\ttotal: 9.74ms\tremaining: 29.2ms\n",
      "25:\tlearn: 3.5930083\ttotal: 10.1ms\tremaining: 28.7ms\n",
      "26:\tlearn: 3.5652965\ttotal: 10.5ms\tremaining: 28.3ms\n",
      "27:\tlearn: 3.5521025\ttotal: 10.8ms\tremaining: 27.8ms\n",
      "28:\tlearn: 3.5200095\ttotal: 11.2ms\tremaining: 27.4ms\n",
      "29:\tlearn: 3.4992667\ttotal: 11.5ms\tremaining: 26.9ms\n",
      "30:\tlearn: 3.4877353\ttotal: 11.9ms\tremaining: 26.5ms\n",
      "31:\tlearn: 3.4627469\ttotal: 12.3ms\tremaining: 26ms\n",
      "32:\tlearn: 3.4567673\ttotal: 12.6ms\tremaining: 25.6ms\n",
      "33:\tlearn: 3.4432513\ttotal: 13ms\tremaining: 25.2ms\n",
      "34:\tlearn: 3.4248841\ttotal: 13.3ms\tremaining: 24.7ms\n",
      "35:\tlearn: 3.4120077\ttotal: 13.7ms\tremaining: 24.3ms\n",
      "36:\tlearn: 3.4020686\ttotal: 14ms\tremaining: 23.9ms\n",
      "37:\tlearn: 3.3928588\ttotal: 14.4ms\tremaining: 23.5ms\n",
      "38:\tlearn: 3.3882415\ttotal: 14.8ms\tremaining: 23.1ms\n",
      "39:\tlearn: 3.3807260\ttotal: 15.1ms\tremaining: 22.7ms\n",
      "40:\tlearn: 3.3726672\ttotal: 15.5ms\tremaining: 22.3ms\n",
      "41:\tlearn: 3.3503646\ttotal: 15.9ms\tremaining: 22ms\n",
      "42:\tlearn: 3.3375152\ttotal: 16.3ms\tremaining: 21.6ms\n",
      "43:\tlearn: 3.3276916\ttotal: 16.6ms\tremaining: 21.2ms\n",
      "44:\tlearn: 3.3235793\ttotal: 17.1ms\tremaining: 20.9ms\n",
      "45:\tlearn: 3.3198063\ttotal: 17.4ms\tremaining: 20.5ms\n",
      "46:\tlearn: 3.3021184\ttotal: 17.8ms\tremaining: 20.1ms\n",
      "47:\tlearn: 3.2837730\ttotal: 18.2ms\tremaining: 19.7ms\n",
      "48:\tlearn: 3.2682016\ttotal: 18.5ms\tremaining: 19.3ms\n",
      "49:\tlearn: 3.2577614\ttotal: 18.9ms\tremaining: 18.9ms\n",
      "50:\tlearn: 3.2476265\ttotal: 19.2ms\tremaining: 18.5ms\n",
      "51:\tlearn: 3.2365662\ttotal: 19.6ms\tremaining: 18.1ms\n",
      "52:\tlearn: 3.2277927\ttotal: 19.9ms\tremaining: 17.7ms\n",
      "53:\tlearn: 3.2212113\ttotal: 20.3ms\tremaining: 17.3ms\n",
      "54:\tlearn: 3.2119660\ttotal: 20.7ms\tremaining: 16.9ms\n",
      "55:\tlearn: 3.2034460\ttotal: 21ms\tremaining: 16.5ms\n",
      "56:\tlearn: 3.1970252\ttotal: 21.4ms\tremaining: 16.1ms\n",
      "57:\tlearn: 3.1933879\ttotal: 21.7ms\tremaining: 15.7ms\n",
      "58:\tlearn: 3.1803705\ttotal: 22.1ms\tremaining: 15.4ms\n",
      "59:\tlearn: 3.1785453\ttotal: 22.5ms\tremaining: 15ms\n",
      "60:\tlearn: 3.1633120\ttotal: 22.8ms\tremaining: 14.6ms\n",
      "61:\tlearn: 3.1562632\ttotal: 23.2ms\tremaining: 14.2ms\n",
      "62:\tlearn: 3.1503533\ttotal: 23.5ms\tremaining: 13.8ms\n",
      "63:\tlearn: 3.1455412\ttotal: 23.9ms\tremaining: 13.4ms\n",
      "64:\tlearn: 3.1325904\ttotal: 24.3ms\tremaining: 13.1ms\n",
      "65:\tlearn: 3.1269965\ttotal: 24.6ms\tremaining: 12.7ms\n",
      "66:\tlearn: 3.1166609\ttotal: 25ms\tremaining: 12.3ms\n",
      "67:\tlearn: 3.1081037\ttotal: 25.3ms\tremaining: 11.9ms\n",
      "68:\tlearn: 3.1024595\ttotal: 25.7ms\tremaining: 11.5ms\n",
      "69:\tlearn: 3.0954191\ttotal: 26ms\tremaining: 11.2ms\n",
      "70:\tlearn: 3.0826566\ttotal: 26.4ms\tremaining: 10.8ms\n",
      "71:\tlearn: 3.0727324\ttotal: 26.8ms\tremaining: 10.4ms\n",
      "72:\tlearn: 3.0628198\ttotal: 27.1ms\tremaining: 10ms\n",
      "73:\tlearn: 3.0596027\ttotal: 27.5ms\tremaining: 9.65ms\n",
      "74:\tlearn: 3.0546325\ttotal: 27.8ms\tremaining: 9.28ms\n",
      "75:\tlearn: 3.0457006\ttotal: 28.2ms\tremaining: 8.9ms\n",
      "76:\tlearn: 3.0427694\ttotal: 28.5ms\tremaining: 8.52ms\n",
      "77:\tlearn: 3.0343826\ttotal: 28.9ms\tremaining: 8.15ms\n",
      "78:\tlearn: 3.0301855\ttotal: 29.3ms\tremaining: 7.78ms\n",
      "79:\tlearn: 3.0181591\ttotal: 29.6ms\tremaining: 7.41ms\n",
      "80:\tlearn: 3.0149767\ttotal: 30ms\tremaining: 7.04ms\n",
      "81:\tlearn: 3.0082759\ttotal: 30.4ms\tremaining: 6.67ms\n",
      "82:\tlearn: 2.9996155\ttotal: 30.7ms\tremaining: 6.29ms\n",
      "83:\tlearn: 2.9969249\ttotal: 31.1ms\tremaining: 5.92ms\n",
      "84:\tlearn: 2.9879295\ttotal: 31.4ms\tremaining: 5.55ms\n",
      "85:\tlearn: 2.9852046\ttotal: 31.8ms\tremaining: 5.18ms\n",
      "86:\tlearn: 2.9787710\ttotal: 32.2ms\tremaining: 4.81ms\n",
      "87:\tlearn: 2.9753461\ttotal: 32.5ms\tremaining: 4.44ms\n",
      "88:\tlearn: 2.9632152\ttotal: 32.9ms\tremaining: 4.06ms\n",
      "89:\tlearn: 2.9590246\ttotal: 33.2ms\tremaining: 3.69ms\n",
      "90:\tlearn: 2.9563759\ttotal: 33.6ms\tremaining: 3.32ms\n",
      "91:\tlearn: 2.9487079\ttotal: 34ms\tremaining: 2.95ms\n",
      "92:\tlearn: 2.9388117\ttotal: 34.3ms\tremaining: 2.58ms\n",
      "93:\tlearn: 2.9376405\ttotal: 34.7ms\tremaining: 2.21ms\n",
      "94:\tlearn: 2.9342663\ttotal: 35ms\tremaining: 1.84ms\n",
      "95:\tlearn: 2.9226185\ttotal: 35.4ms\tremaining: 1.48ms\n",
      "96:\tlearn: 2.9145151\ttotal: 35.8ms\tremaining: 1.11ms\n",
      "97:\tlearn: 2.8995546\ttotal: 36.1ms\tremaining: 737us\n",
      "98:\tlearn: 2.8925059\ttotal: 36.5ms\tremaining: 368us\n",
      "99:\tlearn: 2.8899616\ttotal: 36.8ms\tremaining: 0us\n",
      "RMSE (CatBoost): 4.147\n"
     ]
    }
   ],
   "source": [
    "# Build and fit a CatBoost regressor\n",
    "reg_cat = cb.CatBoostRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=500)\n",
    "reg_cat.fit(X_train,y_train)\n",
    "\n",
    "# Calculate the predictions on the set set\n",
    "pred = reg_cat.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_cat = np.sqrt(mean_squared_error(y_test, pred))\n",
    "\n",
    "print('RMSE (CatBoost): {:.3f}'.format(rmse_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8148299",
   "metadata": {},
   "source": [
    "####  Extreme gradient boosting (XGBoost)\n",
    "\n",
    "XGBoost, is a more advanced implementation of the Gradient Boosting algorithm, optimized for distributed computing for both training and prediction phases. \n",
    "\n",
    "While gradient boosting is a sequential ensemble, XGBoost uses parallel processing for training each estimator, thus speeding up the processing. It's described as a scalable, portable, and accurate solution that can work with huge datasets. \n",
    "\n",
    "To build a XGBoost model, we first import the library with the alias xgb. This allows us to call the classes XGBClassifier or XGBRegressor. The parameters are similar to the ones for Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "010b722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a3f5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (XGBBoost): 3.991\n"
     ]
    }
   ],
   "source": [
    "# Build and fit an XGBoost regressor\n",
    "reg_xgb = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='reg:squarederror', random_state=500)\n",
    "reg_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the predictions and evaluate both regressors\n",
    "pred_xgb = reg_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using the RMSE\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, pred_xgb))\n",
    "\n",
    "print('RMSE (XGBBoost): {:.3f}'.format(rmse_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b7dadb",
   "metadata": {},
   "source": [
    "#### Light gradient boosting machine\n",
    "\n",
    "Let's move on to Light Gradient Boosting, or LightGBM, which is a framework developed by Microsoft. Compared to XGBoost, LightGBM provides faster training and higher efficiency. It is also lighter in terms of space and memory usage. Being a distributed algorithm means it's optimized for parallel and GPU processing. LightGBM is useful when you are dealing with big datasets but have speed or memory constraints. \n",
    "\n",
    "In order to train a LightBoost ensemble model, you must import the lightgbm library and alias it as lgb, which stands for Light Gradient Boosting. Then, you can use the LGBMClassifier or LGBMRegressor depending on your problem. \n",
    "\n",
    "The parameters are similar to the ones for Gradient Boosting, except for max depth which is negative one by default, meaning no limit. Therefore, we must specify its value if a limit is desired. After training the model, you can use the fit and predict methods like with any scikit-learn estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "130af633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3efdd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (LGBBoost): 4.250\n"
     ]
    }
   ],
   "source": [
    "# Build and fit a LightGBM regressor\n",
    "reg_lgb = lgb.LGBMRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, objective='mean_squared_error', seed=500)\n",
    "\n",
    "reg_lgb.fit(X_train, y_train)\n",
    "\n",
    "pred_lgb = reg_lgb.predict(X_test)\n",
    "\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test, pred_lgb))\n",
    "\n",
    "print('RMSE (LGBBoost): {:.3f}'.format(rmse_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb68e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Logistic Regression: 5.007\n",
      "RMSE AdaBoost with LR: 4.855\n",
      "RMSE Decision Tree: 4.365\n",
      "RMSE AdaBoost with DT : 4.265\n",
      "RMSE (LGBBoost): 4.250\n",
      "RMSE (CatBoost): 4.147\n",
      "RMSE (XGBBoost): 3.991\n"
     ]
    }
   ],
   "source": [
    "print('RMSE Logistic Regression: {:.3f}'.format(rmse_lr))\n",
    "print('RMSE AdaBoost with LR: {:.3f}'.format(rmse_lr_ada))\n",
    "print('RMSE Decision Tree: {:.3f}'.format(rmse_dt))\n",
    "print('RMSE AdaBoost with DT : {:.3f}'.format(rmse_dt_ada))\n",
    "print('RMSE (LGBBoost): {:.3f}'.format(rmse_lgb))\n",
    "print('RMSE (CatBoost): {:.3f}'.format(rmse_cat))\n",
    "print('RMSE (XGBBoost): {:.3f}'.format(rmse_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d721e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f8a5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9cd90e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7edebb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd4cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35e53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2ec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30b4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
