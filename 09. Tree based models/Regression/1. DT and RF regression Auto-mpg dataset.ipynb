{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce759517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee748b",
   "metadata": {},
   "source": [
    "### Decision-Tree for Regression\n",
    "\n",
    "In regression, the target variable is continuous. In other words, the output of your model is a real value.\n",
    "\n",
    "#### Auto-mpg Dataset\n",
    "\n",
    "Let's motivate our discussion of regression by introducing the automobile miles-per-gallon dataset from the UCI Machine Learning Repository. This dataset consists of 6 features corresponding to the characteristics of a car and a continuous target variable labeled mpg which stands for miles-per-gallon. \n",
    "\n",
    "Our task is to predict the mpg consumption of a car given these six features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83565aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"auto.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4bb664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>displ</th>\n",
       "      <th>hp</th>\n",
       "      <th>weight</th>\n",
       "      <th>accel</th>\n",
       "      <th>origin</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>US</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>60</td>\n",
       "      <td>1800</td>\n",
       "      <td>16.4</td>\n",
       "      <td>Asia</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3525</td>\n",
       "      <td>19.0</td>\n",
       "      <td>US</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.3</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78</td>\n",
       "      <td>2188</td>\n",
       "      <td>15.8</td>\n",
       "      <td>Europe</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  displ   hp  weight  accel  origin  size\n",
       "0  18.0  250.0   88    3139   14.5      US  15.0\n",
       "1   9.0  304.0  193    4732   18.5      US  20.0\n",
       "2  36.1   91.0   60    1800   16.4    Asia  10.0\n",
       "3  18.5  250.0   98    3525   19.0      US  15.0\n",
       "4  34.3   97.0   78    2188   15.8  Europe  10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9d5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"mpg\", \"origin\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36a957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"mpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa935cc",
   "metadata": {},
   "source": [
    "First, import train_test_split() from sklearn-dot-model_selection and mean_squared_error as MSE() from sklearn-dot-metrics. Then, split the data into 80%-train and 20%-test using train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46e9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbc9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484ab8f",
   "metadata": {},
   "source": [
    "#### Regression-Tree in scikit-learn\n",
    "\n",
    "Let's see how you can train a decision tree with scikit-learn to solve this regression problem. Import DecisionTreeRegressor from sklearn-dot-tree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5f2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd32aa",
   "metadata": {},
   "source": [
    "Instantiate the DecisionTreeRegressor() with a maximum depth of 8 by setting the parameter max_depth to 8. In addition, set the parameter min_sample_leaf to 0.13 to impose a stopping condition in which each leaf has to contain at least 13% of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ebb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "             min_samples_leaf=0.13,\n",
    "            random_state=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec763935",
   "metadata": {},
   "source": [
    "Now fit dt to the training set and predict the test set labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19620efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=8, min_samples_leaf=0.13, random_state=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85afad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237cbc22",
   "metadata": {},
   "source": [
    "####  Information Criterion for Regression-Tree\n",
    "\n",
    "Here, it's important to note that, when a regression tree is trained on a dataset, the impurity of a node is measured using the mean-squared error of the targets in that node. This means that the regression tree tries to find the splits that produce leafs where in each leaf the target values are on average, the closest possible to the mean-value of the labels in that particular leaf.\n",
    "\n",
    "To evaluate the test set performance, we'll use the Root Mean Squared Error (RMSE) metric. The RMSE of a model measures, on average, how much the model's predictions differ from the actual labels. The RMSE of a model can be obtained by computing the square root of the model's Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011b85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e4146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt ** (1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d603592d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of dt: 4.47\n"
     ]
    }
   ],
   "source": [
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518753a",
   "metadata": {},
   "source": [
    "#### Linear regression vs regression tree\n",
    "\n",
    "In this exercise, you'll compare the test set RMSE of dt to that achieved by a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30ab68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict test set labels \n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "# Compute mse_lr\n",
    "mse_lr = MSE(y_test, y_pred_lr)\n",
    "\n",
    "# Compute rmse_lr\n",
    "rmse_lr = mse_lr ** (1/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9dca85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression test set RMSE: 5.01\n",
      "Regression Tree test set RMSE: 4.47\n"
     ]
    }
   ],
   "source": [
    "# Print rmse_lr\n",
    "print('Linear Regression test set RMSE: {:.2f}'.format(rmse_lr))\n",
    "\n",
    "# Print rmse_dt\n",
    "print('Regression Tree test set RMSE: {:.2f}'.format(rmse_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f1894",
   "metadata": {},
   "source": [
    "#### Supervised Learning - Under the Hood\n",
    "\n",
    "In supervised learning, you make the assumption that there's a mapping f between features and labels. You can express this as y=f(x). f which is an unknown function that you want to determine. In reality, data generation is always accompanied with randomness or noise.\n",
    "\n",
    "#### Goals of Supervised Learning\n",
    "\n",
    "Your goal is to find a model f'(f' can be Logistic Regression, Decision Tree, Neural Network etc.), that best approximates f. When training f', you want to make sure that noise is discarded as much as possible. At the end, f' should achieve a low predictive error on unseen datasets.\n",
    "\n",
    "#### Difficulties in Approximating f\n",
    "\n",
    "You may encounter two difficulties when approximating f. \n",
    "\n",
    "The first is overfitting, it's when f' fits the noise in the training set. \n",
    "\n",
    "The second is underfitting, it's when f' is not flexible enough to approximate f.\n",
    "\n",
    "#### Overfitting\n",
    "\n",
    "When a model overfits the training set, its predictive power on unseen datasets is pretty low. In such cases, The model memorizes the noise present in the training set. Such model achieves a low training set error and a high test set error.\n",
    "\n",
    "#### Underfitting\n",
    "\n",
    "When a model underfits the data the training set error is roughly equal to the test set error. However, both errors are relatively high. Now the trained model isn't flexible enough to capture the complex dependency between features and labels.\n",
    "\n",
    "#### Generalization Error\n",
    "\n",
    "The generalization error of a model tells you how much it generalizes on unseen data. It can be decomposed into 3 terms: bias, variance and irreducible error where the irreducible error is the error contribution of noise.\n",
    "\n",
    "\n",
    "#### Bias(accurate)\n",
    "\n",
    "The bias term tells you, on average, how much f' and f are different. In this case, the model is not flexible enough to approximate the true function f . High bias models lead to underfitting.\n",
    "\n",
    "#### Variance(precise)\n",
    "\n",
    "The variance term tells you how much f' is inconsistent over different training sets. in this case, f' follows the training data points so closely that it misses the true function f . High variance models lead to overfitting.\n",
    "\n",
    "#### Model Complexity\n",
    "\n",
    "The complexity of a model sets its flexibility to approximate the true function f. For example: increasing the maximum-tree-depth increases the complexity of a decision tree.\n",
    "\n",
    "#### Bias-Variance Tradeoff\n",
    "\n",
    "When the model complexity increases, the variance increases while the bias decreases. \n",
    "\n",
    "Conversely, when model complexity decreases, variance decreases and bias increases. \n",
    "\n",
    "Your goal is to find the model complexity that achieves the lowest generalization error. Since this error is the sum of three terms with the irreducible error being constant, you need to find a balance between bias and variance because as one increases the other decreases. This is known as the bias-variance trade-off.\n",
    "\n",
    "for example, As the complexity of f' increases, the bias term decreases while the variance term increases.\n",
    "\n",
    "####  Estimating the Generalization Error\n",
    "\n",
    "Given that you've trained a supervised machine learning model labeled f', how do you estimate the f' 's generalization error? This cannot be done directly because: \n",
    "\n",
    "- f is unknown, \n",
    "\n",
    "- usually you only have one dataset, \n",
    "\n",
    "- you don't have access to the error term due to noise.\n",
    "\n",
    "A solution to this is to first split the data into a training and test set. The model f' can then be fit to the training set and its error can be evaluated on the test set. The generalization error of f' is roughly approximated by f' s error on the test set. Usually, the test set should be kept untouched until one is confident about f''s performance. It should only be used to evaluate f''s final performance or error. \n",
    "\n",
    "Now, evaluating f''s performance on the training set may produce an optimistic estimation of the error because f' was already exposed to the training set when it was fit. \n",
    "\n",
    "To obtain a reliable estimate of fhat's performance, you should use a technique called cross-validation or CV. CV can be performed using K-Fold-CV or hold-out-CV . \n",
    "\n",
    "#### K-Fold CV\n",
    "\n",
    "For K=10: \n",
    "\n",
    "- First, the training set (T) is split randomly into 10 partitions or folds, \n",
    "\n",
    "- The error of fhat is evaluated 10 times on the 10 folds, \n",
    "\n",
    "- Each time, one fold is picked for evaluation after training fhat on the other 9 folds. \n",
    "\n",
    "- At the end, you'll obtain a list of 10 errors.\n",
    "\n",
    "Finally, the CV-error is computed as the mean of the 10 obtained errors.\n",
    "\n",
    "\n",
    "#### Diagnose Variance Problems\n",
    "\n",
    "Once you have computed f''s cross-validation-error, you can check if it is greater than f''s training set error. If it is greater, f' is said to suffer from high variance. In such case, f' has overfit the training set. To remedy this try decreasing f''s complexity. \n",
    "\n",
    "For example, in a decision tree you can reduce the maximum-tree-depth or increase the maximum-samples-per-leaf. In addition, you may also gather more data to train f'.\n",
    "\n",
    "#### Diagnose Bias Problems\n",
    "\n",
    "On the other hand, f' is said to suffer from high bias if its cross-validation-error is roughly equal to the training error but much greater than the desired error. In such case fhat underfits the training set. To remedy this try increasing the model's complexity(for ex: increase max depth, decrease min samples per leaf) or gather more relevant features for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f8007",
   "metadata": {},
   "source": [
    "####  K-Fold CV in sklearn on the Auto Dataset\n",
    "\n",
    "Let's now see how we can perform K-fold-cross-validation using scikit-learn on the auto-dataset which is already loaded. In addition to the usual imports, you should also import the function cross_val_score() from sklearn-dot-model_selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00c3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259eaa1c",
   "metadata": {},
   "source": [
    "First, split the dataset into 70%-train and 30%-test using train_test_split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa937f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9720b75",
   "metadata": {},
   "source": [
    "Then, instantiate a DecisionTreeRegressor() dt with the parameters max_depth set to 4 and min_samples_leaf to 0.26."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b810833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a DecisionTreeRegressor dt\n",
    "dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.26, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4730eb",
   "metadata": {},
   "source": [
    "Next, call cross_val_score() by passing dt, X_train, y_train; set the parameters cv to 10 for 10-fold-cross-validation and scoring to neg_mean_squared_error to compute the negative-mean-squared-errors. \n",
    "\n",
    "The scoring parameter was set so because cross_val_score() does not allow computing the mean-squared-errors directly. \n",
    "\n",
    "Finally, set n_jobs to -1 to exploit all available CPUs in computation. The result is a numpy-array of the 10 negative mean-squared-errors achieved on the 10-folds. You can multiply the result by minus-one to obtain an array of CV-MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d02d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train, y_train, cv=10, \n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d418ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7d851",
   "metadata": {},
   "source": [
    "After that, fit dt to the training set and evaluate the labels of the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94aae86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train, y_pred_train))**(1/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c6d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of test set\n",
    "y_pred_test = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_test = (MSE(y_test, y_pred_test))**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19e88b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV RMSE: 5.14\n",
      "Train RMSE: 5.15\n",
      "Test RMSE: 4.86\n"
     ]
    }
   ],
   "source": [
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))\n",
    "\n",
    "# Test set MSE\n",
    "print('Test RMSE: {:.2f}'.format(RMSE_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec5782",
   "metadata": {},
   "source": [
    "We know, if cross-validation-error is roughly equal to the training error but much greater than the desired error. In such case underfits the training set.\n",
    "\n",
    "dt is indeed underfitting the training set as the model is too constrained to capture the nonlinear dependencies between features and labels.\n",
    "\n",
    "To remedy this try increasing the model's complexity(for ex: increase max depth, decrease min samples per leaf) or gather more relevant features for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1852d9",
   "metadata": {},
   "source": [
    "### Random Forests Regressor in sklearn (auto dataset)\n",
    "\n",
    "Here, you'll train a random forests regressor to the auto-dataset. The dataset is already loaded. After importing RandomForestRegressor, train_test_split and mean_squared_error as MSE, split the dataset into 70%-train and 30%-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d95a813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e2403",
   "metadata": {},
   "source": [
    "Then instantiate a RandomForestRegressor consisting of 400 regression trees. This can be done by setting n_estimators to 400. \n",
    "\n",
    "In addition, set min_samples_leaf to 0-dot-12 so that each leaf contains at least 12% of the data used in training. \n",
    "\n",
    "You can now fit rf to the training set and predict the test set labels. Finally, print the test set RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcbd7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.12, random_state=SEED)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb24cc56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 3.98\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a27e41",
   "metadata": {},
   "source": [
    "The result shows that rf achieves a test set RMSE of 3.98; this error is smaller than that achieved by a single regression tree which is 4.86."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce1e85",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "When a tree based method is trained, the predictive power of a feature or its importance can be assessed. \n",
    "\n",
    "In scikit-learn, feature importance is assessed by measuring how much the tree nodes use a particular feature to reduce impurity. \n",
    "\n",
    "Note that the importance of a feature is expressed as a percentage indicating the weight of that feature in training and prediction. Once you train a tree-based model in scikit-learn, the features importances can be accessed by extracting the feature_importance_ attribute from the model.\n",
    "\n",
    "#### Feature Importance in sklearn\n",
    "\n",
    "To visualize the importance of features as assessed by rf, you can create a pandas series of the features importances as shown here and then sort this series and make a horiztonal-barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75411bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a pd.Series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a30b7",
   "metadata": {},
   "source": [
    "The results show that, according to rf, displ, size, weight and hp are the most predictive features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c74578",
   "metadata": {},
   "source": [
    "#### Random Forests Hyperparameters\n",
    "\n",
    "In addition to the hyperparameters of the CARTs forming random forests, the ensemble itself is characterized by other hyperparameters such as the number of estimators, whether it uses bootstraping or not and so on.\n",
    "\n",
    "#### Tuning is expensive\n",
    "\n",
    "As a note, hyperparameter tuning is computationally expensive and may sometimes lead only to very slight improvement of a model's performance. For this reason, it is desired to weigh the impact of tuning on the pipeline of your data analysis project as a whole in order to understand if it is worth pursuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080de98",
   "metadata": {},
   "source": [
    "#### Inspecting RF Hyperparameters in sklearn\n",
    "\n",
    "To inspect the hyperparameters of a RandomForestRegressor, first, import RandomForestRegressor from sklearn.ensemble and then instantiate a RandomForestRegressor rf ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3f185de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "# Instantiate a random forests regressor 'rf'\n",
    "rf = RandomForestRegressor(random_state= SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6801e3",
   "metadata": {},
   "source": [
    "#### Inspecting RF Hyperparameters in sklearn\n",
    "\n",
    "The hyperparameters of rf along with their default values can be accessed by calling rf's dot-get_params() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a3a1913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect rf' s hyperparameters\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f956b",
   "metadata": {},
   "source": [
    "In the following, we'll be optimizing n_estimators, max_depth, min_samples_leaf and max_features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f7c3b",
   "metadata": {},
   "source": [
    "### GridSearchCV in sklearn (auto dataset)\n",
    "\n",
    "We'll perform grid-search cross-validation on the auto-dataset which is already loaded and split into 80%-train and 20%-test. \n",
    "\n",
    "First import mean_squared_error as MSE from sklearn.metrics and GridSearchCV from sklearn.model_selection. \n",
    "\n",
    "Then, define a dictionary called params_rf containing the grid of hyperparameters. \n",
    "\n",
    "Finally, instantiate a GridSearchCV object called grid_rf and pass the parameters rf as estimator, params_rf as param_grid. Also set cv to 3 to perform 3-fold cross-validation. \n",
    "\n",
    "In addition, set scoring to neg_mean_squared_error in order to use negative mean squared error as a metric. \n",
    "\n",
    "Note that the parameter verbose controls verbosity; the higher its value, the more messages are printed during fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdca4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da626438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameter 'params_rf'\n",
    "params_rf = {'n_estimators': [300, 400, 500],'max_depth': [4, 6, 8],'min_samples_leaf': [0.1, 0.2],\n",
    "             'max_features': ['log2', 'sqrt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4074232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate 'grid_rf'\n",
    "grid_rf = GridSearchCV(estimator=rf,param_grid=params_rf,cv=3,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2913fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(random_state=1), n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 6, 8],\n",
       "                         'max_features': ['log2', 'sqrt'],\n",
       "                         'min_samples_leaf': [0.1, 0.2],\n",
       "                         'n_estimators': [300, 400, 500]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit 'grid_rf' to the training set\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067729b2",
   "metadata": {},
   "source": [
    "#### Extracting the best hyperparameters\n",
    "\n",
    "You can extract rf's best hyperparameters by getting the attribute best_params_ from grid_rf. \n",
    "\n",
    "#### Evaluating the best model performance\n",
    "\n",
    "You can also extract the best model from rf. This enables you to predict the test set labels and evaluate the test-set RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53c02c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      " {'max_depth': 4, 'max_features': 'log2', 'min_samples_leaf': 0.1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Extract the best hyperparameters from 'grid_rf'\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "\n",
    "print('Best hyperparameters:\\n', best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3822f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE of rf: 3.83\n"
     ]
    }
   ],
   "source": [
    "# Extract the best model from 'grid_rf'\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c451b",
   "metadata": {},
   "source": [
    "The output shows a result of 3.83. If you would have trained an untuned model, the RMSE would be 3.98."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b9a5f",
   "metadata": {},
   "source": [
    "### Gradient Boosting (GB)\n",
    "\n",
    "Gradient Boosting is a popular boosting algorithm that has a proven track record of winning many machine learning competitions.\n",
    "\n",
    "In gradient boosting, each predictor in the ensemble corrects its predecessor's error. In contrast to AdaBoost, the weights of the training instances are not tweaked. Instead, each predictor is trained using the residual errors of its predecessor as labels. \n",
    "\n",
    "#### Gradient Boosted Trees for Regression: Training\n",
    "\n",
    "To understand how gradient boosted trees are trained for a regression problem, take an assumption. \n",
    "\n",
    "Suppose, The ensemble consists of N trees. Tree 1 is trained using the features matrix X and the dataset labels y. The predictions labeled y1 are used to determine the training set residual errors r. where, r = y-y1\n",
    "\n",
    "Tree2 is then trained using the features matrix X and the residual errors r of Tree1 as labels. The predicted residuals r1 are then used to determine the residuals of residuals which are labeled r2. where, r2 = r-r1 \n",
    "\n",
    "This process is repeated until all of the N trees forming the ensemble are trained.\n",
    "\n",
    "#### Shrinkage\n",
    "\n",
    "An important parameter used in training gradient boosted trees is shrinkage. In this context, shrinkage refers to the fact that the prediction of each tree in the ensemble is shrinked after it is multiplied by a learning rate which is a number between 0 and 1. \n",
    "\n",
    "Similarly to AdaBoost, there's a trade-off between and the number of estimators. Decreasing the learning rate needs to be compensated by increasing the number of estimators in order for the ensemble to reach a certain performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2b8d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and utility functions\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63fcffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a GradientBoostingRegressor 'gbt'\n",
    "gbt = GradientBoostingRegressor(n_estimators=25, max_features=0.6,max_depth=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebbb74db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=1, max_features=0.6, n_estimators=25,\n",
       "                          random_state=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit 'gbt' to the training set\n",
    "gbt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0383bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set labels\n",
    "y_pred = gbt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab0b23bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 3.69\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba6730",
   "metadata": {},
   "source": [
    "### Gradient Boosting: Cons\n",
    "\n",
    "Gradient boosting involves an exhaustive search procedure. Each tree in the ensemble is trained to find the best split-points and the best features. This procedure may lead to CARTs that use the same split-points and possibly the same features.\n",
    "\n",
    "####  Stochastic Gradient Boosting\n",
    "\n",
    "To mitigate these effects, you can use an algorithm known as stochastic gradient boosting. \n",
    "\n",
    "In stochastic gradient boosting, each CART is trained on a random subset of the training data. This subset is sampled without replacement. \n",
    "\n",
    "Furthermore, at the level of each node, features are sampled without replacement when choosing the best split-points. As a result, this creates further diversity in the ensemble and the net effect is adding more variance to the ensemble of trees.\n",
    "\n",
    "#### Stochastic Gradient Boosting: Training\n",
    "\n",
    "First, instead of providing all the training instances to a tree, only a fraction of these instances are provided through sampling without replacement. \n",
    "\n",
    "The sampled data is then used for training a tree. However, not all features are considered when a split is made. Instead, only a certain randomly sampled fraction of these features are used for this purpose. \n",
    "\n",
    "Once a tree is trained, predictions are made and the residual errors can be computed. These residual errors are multiplied by the learning rate eta and are fed to the next tree in the ensemble. \n",
    "\n",
    "This procedure is repeated sequentially until all the trees in the ensemble are trained. The prediction procedure for a new instance in stochastic gradient boosting is similar to that of gradient boosting.\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Boosting in sklearn (auto dataset)\n",
    "\n",
    "We'll be dealing with the auto-dataset which is already loaded. \n",
    "\n",
    "Now define a stochastic-gradient-boosting-regressor named sgbt consisting of 300 decision-stumps. This can be done by setting the parameters max_depth to 1 and n_estimators to 300. \n",
    "\n",
    "Set the parameter subsample  to 0.8 in order for each tree to sample 80% of the data for training. \n",
    "\n",
    "Finally, Set the parameter max_features was set to o.8 so that each tree uses 80% of available features to perform the best-split. \n",
    "\n",
    "Once done, fit sgbt to the training set and predict the test set labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7dbbc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a stochastic GradientBoostingRegressor 'sgbt'\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.6, n_estimators=25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "06422982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=1, max_features=0.6, n_estimators=25,\n",
       "                          random_state=1, subsample=0.8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit 'sgbt' to the training set\n",
    "sgbt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30f7660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set labels\n",
    "y_pred = sgbt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3905afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE: 3.63\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test set RMSE 'rmse_test'\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "# Print 'rmse_test'\n",
    "print('Test set RMSE: {:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa061301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22924a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411b6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
